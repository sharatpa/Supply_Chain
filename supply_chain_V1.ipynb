{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supply_chain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZR2ul4X8d1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4181b815-b44b-4ba5-d3a9-e5601ef50e02"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huqnzkuk9W6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Copy contents from My Drive to \"/content\" in order to import all scripts.\n",
        "!cp -r /content/drive/My\\ Drive/SC_RL /content"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3f9ENl3fo_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rm -rf SC_RL/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLl9U2p6EkjZ",
        "colab_type": "text"
      },
      "source": [
        "**Goal of this Project:** Maintain inventory of all products while minimizing operational costs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aS4omrnVaU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "cb96c299-2543-4bd0-cad5-c4677b7e076e"
      },
      "source": [
        "!pip install import_ipynb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=3a462e70f9e1ff7c70ee8a087cde4416642ee95b6a96e0b10eb91af474cb59bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiXti2R48Owo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f8f79d3-8cf1-4b8d-becb-5f26d8996e42"
      },
      "source": [
        "## Import libraries.\n",
        "import import_ipynb\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from SC_RL.Environments import warehouse_store"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/SC_RL/Environments/warehouse_store.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxOGD9neYH4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metadata_file = Path(os.getcwd()+\"/SC_RL/data/instacart-market-basket-analysis/products_metadata.xlsx\")\n",
        "forecast_data = Path(os.getcwd()+\"/SC_RL/data/instacart-market-basket-analysis/scenarios.xlsx\")\n",
        "w = warehouse_store.warehouse_store()\n",
        "warehouse_item_limit = 500"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUrwAiiP9Log",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reward_function(states, actions, quantity_removed, timestep):\n",
        "  '''\n",
        "  Should ensure that inventory is stocked, but at the same time, ensure that\n",
        "  wastage is minimized.\n",
        "  1 - (quantity_restocked/total quantity of products)\n",
        "    - (quantity of expired products/total quantity of products)\n",
        "  '''\n",
        "  ## only check quantity portion when it is thrown away.\n",
        "  p_restocked = np.ndarray.sum(actions)\n",
        "  q_max = np.ndarray.sum(states[:,0])\n",
        "  reward = 1 - (p_restocked+quantity_removed)/q_max\n",
        "  return reward"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLQBpXskMAb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a638841-5087-4988-8cf3-a1055a5a7bae"
      },
      "source": [
        "## Test passing a function as argument here:\n",
        "total_reward = w.simulate(metadata_file,forecast_data,reward_function) # Need not initialize; just simulate.\n",
        "print(total_reward)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-292.4123141298442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_aHTI5ZCCVz",
        "colab_type": "text"
      },
      "source": [
        "## **Actor-Critic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-vbrBY5jCbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Parameters:\n",
        "episodes = 100\n",
        "GAMMA = 0.9     # reward discount in TD error\n",
        "LR_A = 0.001    # learning rate for actor\n",
        "LR_C = 0.01     # learning rate for critic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwhasAdg9GgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Actor-Critic Methods:\n",
        "class Actor(object):\n",
        "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
        "        self.sess = sess\n",
        "\n",
        "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
        "        self.a = tf.placeholder(tf.int32, None, \"act\")\n",
        "        self.td_error = tf.placeholder(tf.float32, None, \"td_error\")  # TD_error\n",
        "\n",
        "        with tf.variable_scope('Actor'):\n",
        "            l1 = tf.layers.dense(\n",
        "                inputs=self.s,\n",
        "                units=20,    # number of hidden units\n",
        "                activation=tf.nn.relu,\n",
        "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
        "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
        "                name='l1'\n",
        "            )\n",
        "\n",
        "            self.acts_prob = tf.layers.dense(\n",
        "                inputs=l1,\n",
        "                units=n_actions,    # output units\n",
        "                activation=tf.nn.softmax,   # get action probabilities\n",
        "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
        "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
        "                name='acts_prob'\n",
        "            )\n",
        "\n",
        "        with tf.variable_scope('exp_v'):\n",
        "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
        "            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss\n",
        "\n",
        "        with tf.variable_scope('train'):\n",
        "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)\n",
        "\n",
        "    def learn(self, s, a, td):\n",
        "        s = s[np.newaxis, :]\n",
        "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
        "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
        "        return exp_v\n",
        "\n",
        "    def choose_action(self, s):\n",
        "        s = s[np.newaxis, :]\n",
        "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
        "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   # return a int\n",
        "\n",
        "\n",
        "class Critic(object):\n",
        "    def __init__(self, sess, n_features, lr=0.01):\n",
        "        self.sess = sess\n",
        "\n",
        "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
        "        self.v_ = tf.placeholder(tf.float32, [1, 1], \"v_next\")\n",
        "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
        "\n",
        "        with tf.variable_scope('Critic'):\n",
        "            l1 = tf.layers.dense(\n",
        "                inputs=self.s,\n",
        "                units=20,  # number of hidden units\n",
        "                activation=tf.nn.relu,  # None\n",
        "                # have to be linear to make sure the convergence of actor.\n",
        "                # But linear approximator seems hardly learns the correct Q.\n",
        "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
        "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
        "                name='l1'\n",
        "            )\n",
        "\n",
        "            self.v = tf.layers.dense(\n",
        "                inputs=l1,\n",
        "                units=1,  # output units\n",
        "                activation=None,\n",
        "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
        "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
        "                name='V'\n",
        "            )\n",
        "\n",
        "        with tf.variable_scope('squared_TD_error'):\n",
        "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
        "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
        "        with tf.variable_scope('train'):\n",
        "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
        "\n",
        "    def learn(self, s, r, s_):\n",
        "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
        "\n",
        "        v_ = self.sess.run(self.v, {self.s: s_})\n",
        "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
        "                                          {self.s: s, self.v_: v_, self.r: r})\n",
        "        return td_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WevZXjUT-Nze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Initialize session:\n",
        "sess = tf.Session()\n",
        "\n",
        "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
        "critic = Critic(sess, n_features=N_F, lr=LR_C)     # we need a good teacher, so the teacher should learn faster than the actor\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "tf.summary.FileWriter(\"logs/\", sess.graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sblFqRVCHx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Training:\n",
        "for ep in range(episodes):\n",
        "  ## Run environment here.\n",
        "  w.simulate(metadata_file,forecast_data,)\n",
        "  '''Plot reward here.'''\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McmHrj4OKew1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Evaluate:"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}