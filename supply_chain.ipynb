{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supply_chain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZR2ul4X8d1V",
        "outputId": "1e6c9c71-9d39-4528-e0b2-308f636587c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huqnzkuk9W6H"
      },
      "source": [
        "## Copy contents from My Drive to \"/content\" in order to import all scripts.\n",
        "!cp -r /content/drive/My\\ Drive/SC_RL /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3f9ENl3fo_2"
      },
      "source": [
        "#rm -rf SC_RL/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aS4omrnVaU-",
        "outputId": "b8b52171-c07d-4097-93f3-746d681ae334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "!pip install import_ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=217117fd3836ae26031a05b892e22487eeb2a8ebf454fe017a7faca7eb59ee15\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiXti2R48Owo",
        "outputId": "d01c1cef-af1b-4caf-d887-675cb73c58c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Import libraries.\n",
        "import import_ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pathlib import Path\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "from SC_RL.Environments import warehouse_store"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/SC_RL/Environments/warehouse_store.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxOGD9neYH4Q"
      },
      "source": [
        "metadata_file = Path(os.getcwd()+\"/SC_RL/data/instacart-market-basket-analysis/products_metadata.xlsx\")\n",
        "forecast_data = Path(os.getcwd()+\"/SC_RL/data/instacart-market-basket-analysis/scenarios.xlsx\")\n",
        "w = warehouse_store.warehouse_store()\n",
        "num_products = 10\n",
        "min_produts = 0\n",
        "max_products = 20 ## Double check."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUrwAiiP9Log"
      },
      "source": [
        "def reward_function(states, actions):\n",
        "  '''\n",
        "  Should ensure that inventory is stocked, but at the same time, ensure that\n",
        "  wastage is minimized.\n",
        "  1 - (quantity_restocked/total quantity of products)\n",
        "    - (quantity of expired products/total quantity of products)\n",
        "  '''\n",
        "  ## only check quantity portion when it is thrown away.\n",
        "  p_restocked = np.ndarray.sum(actions)\n",
        "  q_max = np.ndarray.sum(states[0:num_products,0])\n",
        "  reward = 1 - (p_restocked+states[num_products,0])/q_max\n",
        "  return reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLQBpXskMAb_"
      },
      "source": [
        "## Test passing a function as argument here:\n",
        "# total_reward = w.simulate(metadata_file,forecast_data,reward_function) # Need not initialize; just simulate.\n",
        "# print(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_aHTI5ZCCVz"
      },
      "source": [
        "## **Actor-Critic**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLVGpwxcUWs3"
      },
      "source": [
        "lr_actor = 1e-2\n",
        "lr_critic = 0.01\n",
        "episodes = 75\n",
        "gamma = 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDP-d7g5Krv_"
      },
      "source": [
        "class Critic(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(20,activation='relu')\n",
        "    self.d2 = tf.keras.layers.Dense(10,activation='relu')\n",
        "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    x = self.d2(x)\n",
        "    v = self.v(x)\n",
        "    return v\n",
        "    \n",
        "\n",
        "class Actor(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(20,activation='relu')\n",
        "    self.d2 = tf.keras.layers.Dense(10,activation='relu')\n",
        "    self.a = tf.keras.layers.Dense(num_products,activation='softmax')\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    x = self.d2(x)\n",
        "    a = self.a(x)\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSLO_Gg-bme6"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, gamma = gamma):\n",
        "    self.gamma = gamma\n",
        "    self.a_opt = tf.keras.optimizers.Adam(learning_rate=lr_actor)\n",
        "    self.c_opt = tf.keras.optimizers.Adam(learning_rate=lr_critic)\n",
        "    self.actor = Actor()\n",
        "    self.critic = Critic()\n",
        "\n",
        "  def act(self,state):\n",
        "    prob = self.actor(np.array([state]))\n",
        "    prob = prob.numpy()\n",
        "    #dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "    # dist = tfp.distributions.Normal(loc=0, scale=1)\n",
        "    # dist = tfp.distributions.Normal(prob,scale=1)\n",
        "    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "    action = dist.sample().numpy()\n",
        "    action = action[0][0:num_products]\n",
        "    action = np.reshape(action,(num_products,1))\n",
        "    # out = np.empty((num_products,1))\n",
        "    # for i in range(action.shape[2]):\n",
        "    #   out[i,0] = np.ceil(np.amax(action[:,:,i]))\n",
        "    return action\n",
        "\n",
        "  def actor_loss(self, prob, action, td):\n",
        "    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "    log_prob = dist.log_prob(action)\n",
        "    # print(\"here\")\n",
        "    log_prob = tf.clip_by_value(log_prob, -1e3, 1e3)\n",
        "    log_prob = tf.where(tf.math.is_nan(log_prob), 0, log_prob)\n",
        "    # print(\"log_prob = \",log_prob)\n",
        "    td = tf.squeeze(td)\n",
        "    loss = -log_prob*td\n",
        "    return loss\n",
        "\n",
        "  def learn(self, state, action, reward, next_state, done):\n",
        "    state = np.array([state])\n",
        "    next_state = np.array([next_state])\n",
        "    with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "      p = self.actor(state, training=True)\n",
        "      v =  self.critic(state,training=True)\n",
        "      vn = self.critic(next_state, training=True)\n",
        "      td = reward + self.gamma*vn*(1-int(done)) - v\n",
        "      # td = reward + self.gamma*vn - v\n",
        "      # print(\"td = \",td)\n",
        "      a_loss = self.actor_loss(p, action, td)\n",
        "      # print(\"actor loss = \",a_loss)\n",
        "      c_loss = td**2\n",
        "    grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
        "    grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
        "    self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
        "    self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
        "    return a_loss, c_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sblFqRVCHx_",
        "outputId": "58b5cb8c-8424-43e8-f4e6-936207a2f2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        }
      },
      "source": [
        "## Training:\n",
        "scrl = Agent()\n",
        "tot_reward_vs_episode = []\n",
        "for ep in range(episodes):\n",
        "  total_reward_per_episode = 0\n",
        "  states = w.reset(metadata_file, forecast_data)\n",
        "  ## get_demand() returns a pre-determined demand of each product for timestep.\n",
        "  demand = w.get_demand()\n",
        "  all_aloss = []\n",
        "  all_closs = []\n",
        "  current_timestep = 0\n",
        "  print(\"Episode number = \",ep)\n",
        "  for current_timestep in range(w.simulation_duration):\n",
        "    ## Sample action according to current policy\n",
        "    action = scrl.act(states)\n",
        "    # print(\"current timestep = \",current_timestep)\n",
        "    ## Execute action and observe reward & next state from E\n",
        "    next_state, reward, done = w.step(action,demand,current_timestep,reward_function)\n",
        "    aloss, closs = scrl.learn(states, action, reward, next_state, done)\n",
        "    all_aloss.append(aloss)\n",
        "    all_closs.append(closs)\n",
        "    # print(\"reward = \",reward)\n",
        "    total_reward_per_episode += reward\n",
        "    next_state = states\n",
        "\n",
        "  tot_reward_vs_episode.append(total_reward_per_episode)\n",
        "  '''Plot reward here.'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode number =  0\n",
            "WARNING:tensorflow:Layer actor is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "WARNING:tensorflow:Layer critic is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Episode number =  1\n",
            "Episode number =  2\n",
            "Episode number =  3\n",
            "Episode number =  4\n",
            "Episode number =  5\n",
            "Episode number =  6\n",
            "Episode number =  7\n",
            "Episode number =  8\n",
            "Episode number =  9\n",
            "Episode number =  10\n",
            "Episode number =  11\n",
            "Episode number =  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-018c94de1d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m## Execute action and observe reward & next state from E\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdemand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_timestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0maloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mall_aloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mall_closs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e0bf36f91135>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;31m# td = reward + self.gamma*vn - v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0;31m# print(\"td = \",td)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0ma_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0;31m# print(\"actor loss = \",a_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e0bf36f91135>\u001b[0m in \u001b[0;36mactor_loss\u001b[0;34m(self, prob, action, td)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# print(\"here\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m     \"\"\"\n\u001b[0;32m--> 964\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/distribution.py\u001b[0m in \u001b[0;36m_call_log_prob\u001b[0;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_log_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/categorical.py\u001b[0m in \u001b[0;36m_log_prob\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    288\u001b[0m         k, logits, base_dtype=dtype_util.base_dtype(self.dtype))\n\u001b[1;32m    289\u001b[0m     return -tf.nn.sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m--> 290\u001b[0;31m         labels=k, logits=logits)\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits_v2\u001b[0;34m(labels, logits, name)\u001b[0m\n\u001b[1;32m   4174\u001b[0m   \"\"\"\n\u001b[1;32m   4175\u001b[0m   return sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 4176\u001b[0;31m       labels=labels, logits=logits, name=name)\n\u001b[0m\u001b[1;32m   4177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   4115\u001b[0m       \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m       cost, _ = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 4117\u001b[0;31m           precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   4118\u001b[0m       \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4119\u001b[0m       \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_static_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m  11216\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11217\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11218\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11219\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11220\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 10 which is outside the valid range of [0, 10).  Label values: 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 [Op:SparseSoftmaxCrossEntropyWithLogits]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxNSF3FEHQMf"
      },
      "source": [
        "tot_reward_vs_episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lvuF4YjR--C",
        "outputId": "ec49f5af-ec76-4b15-f91d-69e4e9380c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "episode_no = np.arange(episodes)\n",
        "plt.plot(episode_no,tot_reward_vs_episode)\n",
        "plt.xlabel(\"Number of episodes\")\n",
        "plt.ylabel(\"Accumulated reward in each episode\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accumulated reward in each episode')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4y37826HkbC"
      },
      "source": [
        "Define the Actor-Critic by using a single neural network. The actor will generate the action probabilities, while the critic will evaluate them.\n",
        "\n",
        "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value  V , which models the state-dependent value function. The goal is to train a model that chooses actions based on a policy    that maximizes expected return."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpXJYN_yHq1I"
      },
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common(inputs)\n",
        "    return self.actor(x), self.critic(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZ15W3PJA6v"
      },
      "source": [
        "num_actions = num_products\n",
        "num_hidden_units = 128\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79CNnK0WLZch"
      },
      "source": [
        "Training the RL-agent using AC involves the following steps:\n",
        "1. Run the agent on the environment to collect training data per episode.\n",
        "2. Compute expected return (reward) at each time step.\n",
        "3. Compute the loss for the combined actor-critic model.\n",
        "4. Compute gradients and update network parameters.\n",
        "5. Repeat 1-4 until either success criterion or max episodes (this is used) has been reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuFw0y5IQD93"
      },
      "source": [
        "### 1. Collecting training data\n",
        "\n",
        "As in supervised learning, in order to train the actor-critic model, we need to have training data. However, in order to collect such data, the model would need to be \"run\" in the environment.\n",
        "\n",
        "We collect training data for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.\n",
        "\n",
        "The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated.\n",
        "\n",
        "This process is implemented in the run_episode function, which uses TensorFlow operations so that it can later be compiled into a TensorFlow graph for faster training. Note that tf.TensorArrays were used to support Tensor iteration on variable length arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-6AfH-oRQKr"
      },
      "source": [
        "# Wrap warehouse_environment's `step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, _ = w.step(action)\n",
        "  return (state.astype(np.float32), \n",
        "          np.array(reward, np.int32),\n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], \n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iki6-fShR82z"
      },
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> List[tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "  \n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  values = dict()\n",
        "  action_probs = dict()\n",
        "  rewards = dict()\n",
        "  for t in tf.range(w.simulation_duration):\n",
        "    ## Convert state into a batched tensor (batch size = 1)\n",
        "    # state = tf.expand_dims(state, 0)\n",
        "    ## Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "    ## Sample next action from the action probability distribution\n",
        "    # action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action = tf.random.categorical(action_logits_t, 10)\n",
        "    print(\"action = \",action)\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "    print(\"action_probs_t = \",action_probs_t)\n",
        "    # Store critic values\n",
        "    values[t] = value\n",
        "    # values = values.write(t, tf.squeeze(value))\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs[t] = action_probs_t\n",
        "    # action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    ## should be tf_env_step(action,demand,current_timestep,reward_function)\n",
        "    state.set_shape(initial_state_shape)\n",
        "  \n",
        "    # Store reward\n",
        "    rewards[t] = reward\n",
        "    # rewards = rewards.write(t, reward)\n",
        "\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  # action_probs = action_probs.stack()\n",
        "  # values = values.stack()\n",
        "  # rewards = rewards.stack()\n",
        "\n",
        "  return action_probs, values, rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkpOVpL2VGCQ"
      },
      "source": [
        "### 2. Computing expected returns\n",
        "\n",
        "We convert the sequence of rewards for each timestep $t$, $\\{r_{t}\\}^{T}_{t=1}$ collected during one episode into a sequence of expected returns $\\{G_{t}\\}^{T}_{t=1}$ in which the sum of rewards is taken from the current timestep $t$ to $T$ and each reward is multiplied with an exponentially decaying discount factor $\\gamma$:\n",
        "\n",
        "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
        "\n",
        "Since $\\gamma\\in(0,1)$, rewards further out from the current timestep are given less weight.\n",
        "\n",
        "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n",
        "\n",
        "To stabilize training, we also standardize the resulting sequence of returns (i.e. to have zero mean and unit standard deviation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPynuAdcYsrx"
      },
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float, \n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKEBwcrDY4EA"
      },
      "source": [
        "### 3. The actor-critic loss\n",
        "\n",
        "Since we are using a hybrid actor-critic model, we will use loss function that is a combination of actor and critic losses for training, as shown below:\n",
        "\n",
        "$$L = L_{actor} + L_{critic}$$\n",
        "\n",
        "#### Actor loss\n",
        "\n",
        "We formulate the actor loss based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and compute single-sample (per-episode) estimates.\n",
        "\n",
        "$$L_{actor} = -\\sum^{T}_{t=1} log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
        "\n",
        "where:\n",
        "- $T$: the number of timesteps per episode, which can vary per episode\n",
        "- $s_{t}$: the state at timestep $t$\n",
        "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
        "- $\\pi_{\\theta}$: is the policy (actor) parameterized by $\\theta$\n",
        "- $V^{\\pi}_{\\theta}$: is the value function (critic) also parameterized by $\\theta$\n",
        "- $G = G_{t}$: the expected return for a given state, action pair at timestep $t$\n",
        "\n",
        "We add a negative term to the sum since we want to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
        "\n",
        "<br>\n",
        "\n",
        "##### Advantage\n",
        "\n",
        "The $G - V$ term in our $L_{actor}$ formulation is called the [advantage](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), which indicates how much better an action is given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
        "\n",
        "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic $V$ as a baseline is that it trained to be as close as possible to $G$, leading to a lower variance.\n",
        "\n",
        "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
        "\n",
        "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective $J$. With the critic, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Critic loss\n",
        "\n",
        "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following loss function:\n",
        "\n",
        "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
        "\n",
        "where $L_{\\delta}$ is the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is less sensitive to outliers in data than squared-error loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkqcHt6EY6xC"
      },
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKVm0KFHacLM"
      },
      "source": [
        "### 4. Defining the training step to update parameters\n",
        "\n",
        "We combine all of the steps above into a training step that is run every episode. All steps leading up to the loss function are executed with the `tf.GradientTape` context to enable automatic differentiation.\n",
        "\n",
        "We use the Adam optimizer to apply the gradients to the model parameters.\n",
        "\n",
        "We also compute the sum of the undiscounted rewards, `episode_reward`, in this step which would be used later on to evaluate if we have met the success criterion.\n",
        "\n",
        "We apply the `tf.function` context to the `train_step` function so that it can be compiled into a callable TensorFlow graph, which can lead to 10x speedup in training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejSZEjyWadwy"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "    print(\"Run episode of train step.\")\n",
        "\n",
        "    # Calculate expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eXzjyukaiLg"
      },
      "source": [
        "### 5. Run the training loop\n",
        "\n",
        "We execute training by run the training step until either the success criterion or maximum number of episodes is reached.  \n",
        "\n",
        "We keep a running record of episode rewards using a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency. \n",
        "\n",
        "Depending on your runtime, training can finish in less than a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK23gKK_bHbm"
      },
      "source": [
        "%%time\n",
        "\n",
        "max_episodes = 2\n",
        "max_steps_per_episode = w.simulation_duration\n",
        "\n",
        "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
        "# consecutive trials\n",
        "# reward_threshold = 195\n",
        "running_reward = 0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    # initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    initial_state = tf.constant(w.reset(metadata_file, forecast_data), dtype=tf.int32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "    # running_reward = episode_reward*0.01 + running_reward*.99\n",
        "    running_reward += episode_reward\n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "  \n",
        "    # Show average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "  \n",
        "    # if running_reward > reward_threshold:  \n",
        "    #     break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijVaaiPUD8PJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}